{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8400c3a",
   "metadata": {},
   "source": [
    "### Graph Analytics &ndash; Assignment # 2\n",
    "In this assignment, you will exercise the functionality provided by the [**networkx**](https://networkx.org/documentation/latest/index.html) python package to:\n",
    "1. Analyze degree distributions for networks.\n",
    "2. Conduct path analyses on networks.\n",
    "3. Conduct centrality analyses on networks.\n",
    "\n",
    "The datasets used in this assignment come from:\n",
    "1. [Enron email network](http://konect.cc/networks/enron/)\n",
    "2. [Network of airports in the US](https://toreopsahl.com/datasets/#usairports)\n",
    "3. [Python Dependency Network](https://icon.colorado.edu/#!/networks)\n",
    "\n",
    "#### Step-1: Package requirements and imports\n",
    "\n",
    "You might need the following packages installed on your machine to complete this assignment:\n",
    "- networkx\n",
    "- matplotlib\n",
    "- pandas\n",
    "- numpy\n",
    "- json\n",
    "- [powerlaw](https://pypi.org/project/powerlaw/)\n",
    "\n",
    "You can use other Python packages that are part of the Standard Python Library. \n",
    "\n",
    "Set the random seed for numpy.random and random to $74075$. See [NetworkX Randomness](https://networkx.org/documentation/stable/reference/randomness.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1d6551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ebc923",
   "metadata": {},
   "source": [
    "#### Step-2: Create graphs from data\n",
    "\n",
    "The Enron email network dataset has 3 files: README.enron, meta.enron, and out.enron. The data to be loaded is in *out.enron*. One way to create a graph using this data is to use [pandas.read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) to read the edge list into a data frame and then create a [MultiDiGraph](https://networkx.org/documentation/stable/reference/classes/multidigraph.html) using the [from_pandas_edgelist](https://networkx.org/documentation/stable/reference/generated/networkx.convert_matrix.from_pandas_edgelist.html) function.\n",
    "\n",
    "Use the first column of the data as the source node, the second column as the target node, and the fourth column as the *edge_key* in from_pandas_edgelist. The created MultiDiGraph should have 87273 nodes and 1148072 edges. Use can use [info](https://networkx.org/documentation/stable/reference/generated/networkx.classes.function.info.html) to verify that. After the graph has been created, save it to a file called *enron.graphml* using [write_graphml](https://networkx.org/documentation/stable/reference/readwrite/generated/networkx.readwrite.graphml.write_graphml.html).\n",
    "\n",
    "\n",
    "The Airport network dataset has 2 files: openflights_airports.txt and openflights.txt. Both these files need to be used to create the graph. One way to create the graph is to use [pandas.read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) to read both the node list and the edge list into data frames and then use [add_nodes_from](https://networkx.org/documentation/stable/reference/classes/generated/networkx.Graph.add_nodes_from.html) and [add_weighted_edges_from](https://networkx.org/documentation/stable/reference/classes/generated/networkx.Graph.add_weighted_edges_from.html) to create a [Graph](https://networkx.org/documentation/stable/reference/classes/graph.html). \n",
    "\n",
    "From *openflights_airports.txt*, use \"Airport ID\" to create the nodes and add data from the following columns as attributes on the nodes: Name, City, Country, IATA/FAA, Latitude, Longitude. From *openflights.txt*, use the first column as the source node, the second column as the target node, and the third column as the weight. The created Graph should have 6630 nodes and 30501 edges. After the graph has been created, save it to a file called *airports.graphml* using [write_graphml](https://networkx.org/documentation/stable/reference/readwrite/generated/networkx.readwrite.graphml.write_graphml.html).\n",
    "\n",
    "The Python Dependency Network dataset has 1 file: requirements.csv. One way to create the graph is to use [pandas.read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) to read file into a data frame and then use [add_nodes_from](https://networkx.org/documentation/stable/reference/classes/generated/networkx.DiGraph.add_nodes_from.html) and [add_edges_from](https://networkx.org/documentation/stable/reference/classes/generated/networkx.DiGraph.add_edges_from.html) to create a [DiGraph](https://networkx.org/documentation/stable/reference/classes/digraph.html). \n",
    "\n",
    "The data uses a mixed format with some rows representing only a node and some rows representing an edge; some rows have an empty value for the *requirement* column while others do not. Use data from the \"package\" column to create the nodes. Add data from \"package_name\" and \"package_version\" as attributes on the nodes with names *package_name*  and *package_version*. **Note**: these columns might need cleaning. There exist rows in the data for which *package_name* or *package_version* is empty; infer values for these columns from the value of the *package* column. The created DiGraph should have 65395 nodes and 72943 edges. After the graph has been created, save it to a file called *python.graphml* using [write_graphml](https://networkx.org/documentation/stable/reference/readwrite/generated/networkx.readwrite.graphml.write_graphml.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1e264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and serialize graphs here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7499d04e",
   "metadata": {},
   "source": [
    "#### Step-3: Analyzing degree distributions\n",
    "\n",
    "Using [degree](https://networkx.org/documentation/stable/reference/generated/networkx.classes.function.degree.html), [powerlaw.fit](https://pythonhosted.org/powerlaw/#powerlaw.Distribution.fit), [powerlaw.plot_pdf](https://pythonhosted.org/powerlaw/#powerlaw.Distribution.plot_pdf), [powerlaw.fit.plot_pdf](https://pythonhosted.org/powerlaw/#powerlaw.Fit.plot_pdf), and [distribution_compare](https://pythonhosted.org/powerlaw/#powerlaw.Fit.distribution_compare) to test if the degree distribution follows a power law. \n",
    "\n",
    "See [this paper](https://arxiv.org/pdf/1305.0215.pdf) for example code. Save the log-likelihood ratio $R$ and significance $p$ for each network as follows:\n",
    "- Enron: $enron\\_R$ and $enron\\_p$ (in the neighborhood of $3.xyz \\times 10^{5}$ and $1.xyz \\times 10^{-2ab}$)\n",
    "- Python: $python\\_R$ and $python\\_p$ (in the neighborhood of $4.xyz \\times 10^{3}$ and $1.xyz \\times 10^{-1b}$)\n",
    "- Airports: $airports\\_R$ and $airports\\_p$ (in the neighborhood of $4.xyz \\times 10^{2}$ and $1.xyz \\times 10^{-2a}$)\n",
    "\n",
    "Create a dictionary $powerlaw\\_result$ using variables created above:\n",
    "- the names of the variables defined keys\n",
    "- the values of the variables defined values\n",
    "\n",
    "Use [json.dump](https://docs.python.org/3/library/json.html#json.dump) to write the dictionary $powerlaw\\_result$ to a file (filename: powerlaw\\_result.json).\n",
    "\n",
    "For each network, create a plot that shows the theoretical and observed probability distribution functions. Save the plots to *enron.png*, *python.png*, and *airports.png* respectively. The images should look something like this:\n",
    "\n",
    "| Enron | Python | Airports |\n",
    "| :---: | :---: | :---: |\n",
    "| ![enron](enron.png) | ![python](python.png) | ![airports](airports.png) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc11d68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze degree distributions here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67588339",
   "metadata": {},
   "source": [
    "#### Step-4: Analyze the Enron email network\n",
    "\n",
    "Use [degree_centrality](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.degree_centrality.html) to identify the top 10 people (nodes) who deal with the most email traffic ($traffic = emails\\_received + emails\\_sent$). Store the node IDs in descending order of importance into $enron\\_traffic\\_top\\_10$.\n",
    "\n",
    "Use [in_degree_centrality](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.in_degree_centrality.html) to identify the top 10 people who receive the most emails. Store the node IDs in descending order of importance into $enron\\_recipient\\_top\\_10$.\n",
    "\n",
    "Use [out_degree_centrality](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.out_degree_centrality.html) to identify the top 10 people who send the most emails. Store the node IDs in descending order of importance into $enron\\_sender\\_top\\_10$.\n",
    "\n",
    "Use [voterank](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.voterank.html) to identify the top 10 decentralized influencers. See [voterank paper](https://www.nature.com/articles/srep27823) for more details. Store the node IDs in descending order of importance into $enron\\_influential\\_top\\_10$.\n",
    "\n",
    "Identify nodes present in $enron\\_influential\\_top\\_10$ but not present in $enron\\_recipient\\_top\\_10$ or $enron\\_sender\\_top\\_10$ and store into $enron\\_surprises$.\n",
    "\n",
    "Create a dictionary $enron\\_result$ using variables created above:\n",
    "- the names of the variables as keys\n",
    "- the values of the variables as values\n",
    "\n",
    "Use [json.dump](https://docs.python.org/3/library/json.html#json.dump) to write the dictionary $enron\\_result$ to a file (filename: enron\\_result.json).\n",
    "\n",
    "Questions to ponder:\n",
    "- if a person emails themselves frequently, should that count towards importance?\n",
    "- if a person is on a CC list or a BCC list, should that modify the \"weight\" of the edge?\n",
    "- if a person is added to an thread at some point or removed at some point, what might that mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb2c8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze enron email network here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23bf4a1",
   "metadata": {},
   "source": [
    "#### Step-5: Analyze the Python dependency network\n",
    "\n",
    "Use [in_degree_centrality](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.in_degree_centrality.html) to identify the top 5 most used Python packages. Store the node IDs in descending order of importance into $python\\_most\\_used\\_top\\_5$.\n",
    "\n",
    "Use [out_degree_centrality](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.out_degree_centrality.html) to identify the top 5 packages that depend the most on other packages. Store the node IDs in descending order of importance into $python\\_most\\_dependent\\_top\\_5$.\n",
    "\n",
    "Use [eigenvector_centrality](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.eigenvector_centrality.html) to identify the top 5 most important Python packages. Store the node IDs in descending order of importance into $python\\_eigen\\_top\\_5$.\n",
    "\n",
    "Use [katz_centrality](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.katz_centrality.html) to identify the top 5 most important Python packages. Store the node IDs in descending order of importance into $python\\_katz\\_top\\_5$.\n",
    "\n",
    "$python\\_most\\_used\\_top\\_5$, $python\\_eigen\\_top\\_5$, and $python\\_katz\\_top\\_5$ should be identical. Why could that be? It is probably because of the way Python dependency graph was constructed. The values in the \"package\" column are versioned, and the values in the \"requirement\" column are not. The probably means that we constructed a [bipartite](https://mathworld.wolfram.com/BipartiteGraph.html) graph. This representation does not readily capture transitive dependencies between packages.\n",
    "\n",
    "Check if the original graph (say $python\\_graph$) is bipartite using [is_bipartite](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.bipartite.basic.is_bipartite.html) and store the result in $is\\_bipartite$. If the graph is bipartite, all the (shortest) paths to a given node will be of length $1$ (because this is not a weighted graph). \n",
    "\n",
    "Create a modified version of the graph (say $python\\_graph\\_modified$) such that it captures transitive dependencies. Use [relabel_nodes](https://networkx.org/documentation/stable/reference/generated/networkx.relabel.relabel_nodes.html) change the labels of nodes from the value derived from the \"package\" column to the value stored in the \"package_name\" attribute (i.e. without version). **Note**: there are approximately 8500+ packages that appear in the requirement column but not in the \"package\" column. Skip relabeling these nodes by not including them in the mapping provided to relabel_nodes. The modified graph should be DiGraph with 59058 nodes and 72943 edges. Save $python\\_graph\\_modified$ to a file called *python_modified.graphml* using [write_graphml](https://networkx.org/documentation/stable/reference/readwrite/generated/networkx.readwrite.graphml.write_graphml.html).\n",
    "\n",
    "Using $python\\_graph\\_modified$, for each package in $python\\_most\\_used\\_top\\_5$, create a dictionary with the following keys and values:\n",
    "- key: \"direct_dependency_count\"; value: number of packages that directly depend on the package. See [in_degree](https://networkx.org/documentation/stable/reference/classes/generated/networkx.DiGraph.in_degree.html).\n",
    "- key: \"transitive_dependency_count\"; value: total number of packages that depend on this package directly or indirectly. See [single_target_shortest_path](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.shortest_paths.unweighted.single_target_shortest_path.html)\n",
    "\n",
    "Create a dictionary $dependency\\_counts$ with keys being package names (in $python\\_most\\_used\\_top\\_5$) and the values being the corresponding dictionaries created above. \n",
    "\n",
    "Using $python\\_graph\\_modified$:\n",
    "- compute top 5 most used packages and store in $modified\\_python\\_most\\_used\\_top\\_5$\n",
    "- compute top 5 packages that depend the most on other packages and store in $modified\\_python\\_most\\_dependent\\_top\\_5$\n",
    "- compute top 5 most important packages according to Eigenvector centrality and store in $modified\\_python\\_eigen\\_top\\_5$\n",
    "- compute top 5 most important packages according to Katz centrality and store in $modified\\_python\\_katz\\_top\\_5$\n",
    "\n",
    "Are $modified\\_python\\_most\\_used\\_top\\_5$, $modified\\_python\\_eigen\\_top\\_5$, and $modified\\_python\\_katz\\_top\\_5$ identical?\n",
    "\n",
    "Create a dictionary $python\\_result$ using variables created above (except $python\\_graph\\_modified$):\n",
    "- the names of the variables as keys\n",
    "- the values of the variables as values\n",
    "\n",
    "Use [json.dump](https://docs.python.org/3/library/json.html#json.dump) to write the dictionary $python\\_result$ to a file (filename: python\\_result.json).\n",
    "\n",
    "Questions to ponder:\n",
    "- do measures like betweenness centrality and closeness centrality have valid interpretations in this problem?\n",
    "- how can the importance measures generated here be translated into actionable insights for the community that uses these packages?\n",
    "- what other networks would you super-impose on this network to produce even more insights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4eb4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze python dependency network here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d515f68a",
   "metadata": {},
   "source": [
    "#### Step-6: Analyze the Airports network\n",
    "\n",
    "Use [degree_centrality](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.degree_centrality.html) to identify the top 10 aiports. Store the node IDs in descending order of importance into $airports\\_degree\\_top\\_10$.\n",
    "\n",
    "Use [eigenvector_centrality](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.eigenvector_centrality.html) to identify the top 10 aiports. Store the node IDs in descending order of importance into $airports\\_eigenvector\\_top\\_10$.\n",
    "\n",
    "Use [closeness_centrality](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.closeness_centrality.html) to identify the top 10 aiports. Store the node IDs in descending order of importance into $airports\\_closeness\\_top\\_10$.\n",
    "\n",
    "Use [betweenness_centrality](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.betweenness_centrality.html) to identify the top 10 aiports. Store the node IDs in descending order of importance into $airports\\_betweenness\\_top\\_10$.\n",
    "\n",
    "Compute the set union of all these airports and store in $airports\\_top\\_10\\_union$. Compute the set intersection of all these airports and store in $airports\\_top\\_10\\_intersection$. \n",
    "\n",
    "Create a dictionary called $union\\_airports$ the following key-value pairs for each airport in $airports\\_top\\_10\\_union$:\n",
    "- key: node ID\n",
    "- value: dictionary containing:\n",
    "   - keys: \"City\", \"Country\", \"IATA/FAA\"\n",
    "   - values: airport attribute values corresponding to the keys\n",
    "\n",
    "Create a similar dictionary called $intersection\\_airports$ for $airports\\_top\\_10\\_intersection$.\n",
    "\n",
    "Use [nodes](https://networkx.org/documentation/stable/reference/classes/generated/networkx.Graph.nodes.html) with *data* set to True to get the node representing \"Tulsa Intl\" airport and set it to $tulsa\\_airport$. Store the \"Tulsa Intl\" airport ID to $tulsa\\_airport\\_id$.\n",
    "\n",
    "Create a dictionary called $paths\\_to\\_intersection\\_airports$ with:\n",
    "- keys: airport ID for each airport in $airports\\_top\\_10\\_intersection$\n",
    "- values: shortest path for each airport from \"Tulsa Intl\" airport; use [shortest_path](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.shortest_paths.generic.shortest_path.html); do not set any weights.\n",
    "\n",
    "Create a dictionary $airport\\_result$ using variables created above:\n",
    "- the names of the variables as keys\n",
    "- the values of the variables as values\n",
    "\n",
    "Use [json.dump](https://docs.python.org/3/library/json.html#json.dump) to write the dictionary $airport\\_result$ to a file (filename: airport\\_result.json). **Note**: sets and int64 are not JSON serializable; convert sets to lists and convert int64 to ints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af290ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze airport network here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3058ac",
   "metadata": {},
   "source": [
    "#### Step-7: Submit the following documents.\n",
    "\n",
    "- This file after writing the code as required: \\*.ipynb\n",
    "- A PDF version of this notebook after running all the cells\n",
    "- Files from Step-2\n",
    "  - enron.graphml\n",
    "  - airports.graphml\n",
    "  - python.graphml\n",
    "- Files from Step-3\n",
    "  - enron.png\n",
    "  - airports.png\n",
    "  - python.png\n",
    "  - powerlaw_result.json\n",
    "- Files from Step-4\n",
    "  - enron\\_result.json\n",
    "- Files from Step-5\n",
    "  - python\\_result.json\n",
    "  - modified\\_python.graphml\n",
    "- Files from Step-6\n",
    "  - airport_result.json\n",
    "  \n",
    "Compress all the files into a single zip file and submit the zip file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
